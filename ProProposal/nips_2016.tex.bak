\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{Binary Classification of Imbalanced data from Bosch Production Line}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\section{Data Set}

The data source is an ongoing Kaggle competition: Bosch Production Line Performance (\url{https://www.kaggle.com/c/bosch-production-line-performance/data}). The data represents measurements of parts as they move through Bosch's production lines. The goal is to predict which observations will fail quality control ('Response' = 1). It is a binary classification problem. The raw data is separated by the type of feature they contain: numerical, categorical, and date features. The date features provide a timestamp for when each measurement was taken.

\section{Project Idea}

To solve this binary classification problem, we will follow the basic machine learning pipeline. The first step is to generate new features from raw data(including stratifying and sampling). Then we will select machine models and do hyperparameter selection by grid search or bayes search. In the last, we will ensemble several models based on train data and make a prediction of test data.

We will focus on the following areas: feature engineering, sampling and machine learning model. For feature engineering, we will do data exploration and feature reduction, the size of raw data is large(15.4GB) and doesn't fit into a single machine without feature reduction. The goal is to fit important features into a single memory with 16GB memory. For sampling, we will testify different methods such as upsampling, downsampling and smote to process imbalanced data.  The samples are highly imbalanced(6879:1176868).For machine learning model, gradient boosting tree is popular in Kaggle competitions for structured data. We will firstly use gradient boosting tree and then compare its performance with other machine learning models. Moreover, the shortcoming of gradient boosting tree is speed, it is kind of slow compared to random forest. We will testify three implementations of speeding up gradient boosting tree: Xgboost, FastBDT and SAS Viya. We want to get some insights how to improve the performance of  gradient boosting tree for imbalanced data.

\section{Software}

python open source libraries: pandas, scikit-learn, matplotlib

gradient boosting tree libraries: xgboost and FastBDT

cloud platform: SAS Viya

\section{Papers to read}

Chawla, Nitesh V., et al. "SMOTE: synthetic minority over-sampling technique." Journal of artificial intelligence research 16 (2002): 321-357.

Tianqi Chen, Carlos Guestrin, "XGBoost: A Scalable Tree Boosting System." 2016, KDD.

Keck T. FastBDT: A speed-optimized and cache-friendly implementation of stochastic gradient-boosted decision trees for multivariate classification[J]. arXiv preprint arXiv:1609.06119, 2016.

\bibliography{PaperList}

\section{Teammate and work division}
Xi: Data exploration and feature reduction for categorical data, SMOTE

Liang: SAS Viya machine learning pipeline, Tree building for imbalanced data

Weijie: Employ and compare xgboost and FastBDT, hyperparameter optimization

Yeojin: Data exploration and feature reduction for date data, upsampling and downsampling

\section{Midterm milestone}
The project can be divided into three phases. The first phase is to do feature engineering and get first submission in Kaggle by SAS Viya. The second phase is generate submission by xgboost and FastBDT in single machine with new features. The last phase is to try different sampling methods and get insights how to develop gradient boosting tree for imbalanced data. The goals of the first two phases are midterm milestone.

\end{document}
